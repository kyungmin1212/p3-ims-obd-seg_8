{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58598f31-24e2-4f23-8f00-e159bc9f3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from importlib import import_module\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1890ac93-b01e-46f1-a95f-a061c80a1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/opt/ml/input/data'\n",
    "dataset='CustomDataLoader'\n",
    "num_classes=12\n",
    "test_name='test.json'\n",
    "num_workers=2\n",
    "batch_size=64\n",
    "test_augmentation='TestAugmentation'\n",
    "\n",
    "model0_dir='./model/DeepLabV3Plus_ResNet152_Adamp_0.0001_cutout30_plus_pseudo_cos_softcrossentropyloss0.5_all'\n",
    "model1_dir='./model/DeepLabV3Plus_SeResNext101_Adamp_0.0001_cutout30_plus_pseudo_cos_softcrossentropyloss0.52'\n",
    "\n",
    "\n",
    "output_name='softensemblebest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed6894c-ef4b-449a-b4ce-5d74804b6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f3b682-1b54-42c2-9602-eb47644ac8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/realcode/dataset.py:112: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  new_data=np.array(new_data)\n"
     ]
    }
   ],
   "source": [
    "test_path=os.path.join(data_dir,test_name)\n",
    "dataset_module=getattr(import_module('dataset'),dataset)   # default : CustomDataLoader\n",
    "test_transform_module = getattr(import_module(\"dataset\"),test_augmentation)  # default: TestAugmentation\n",
    "test_transform=test_transform_module()  # augmentation 에 입력값을 넣어주고 싶으면 TestAugmentation을 수정하고 여기에 넣어주면된다.\n",
    "# test dataset\n",
    "test_dataset = dataset_module(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "#                                             num_workers=num_workers,\n",
    "                                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a373de6b-c0f5-405b-b59b-216d66ed997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction.\n",
      "(837, 12, 262144)\n",
      "Start prediction.\n",
      "(837, 12, 262144)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    if i==0:\n",
    "        model_dir=model0_dir\n",
    "        model=smp.DeepLabV3Plus(encoder_name='resnet152',classes=num_classes,encoder_weights='imagenet',activation=None)\n",
    "        \n",
    "    elif i==1:\n",
    "        model_dir=model1_dir\n",
    "        model=smp.DeepLabV3Plus(encoder_name='se_resnext101_32x4d',classes=num_classes,encoder_weights='imagenet',activation=None)\n",
    "#     elif i==2:\n",
    "#         model_dir=model2_dir\n",
    "#     elif i==3:\n",
    "#         model_dir=model3_dir\n",
    "#     elif i==4:\n",
    "#         model_dir=model4_dir\n",
    "    model_path = os.path.join(model_dir, 'best.pth')\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "#     test_path=os.path.join(data_dir,test_name)\n",
    "#     dataset_module=getattr(import_module('dataset'),dataset)   # default : CustomDataLoader\n",
    "#     test_transform_module = getattr(import_module(\"dataset\"),test_augmentation)  # default: TestAugmentation\n",
    "#     test_transform=test_transform_module()  # augmentation 에 입력값을 넣어주고 싶으면 TestAugmentation을 수정하고 여기에 넣어주면된다.\n",
    "#     # test dataset\n",
    "#     test_dataset = dataset_module(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "#     def collate_fn(batch):\n",
    "#         return tuple(zip(*batch))\n",
    "\n",
    "#     test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                             batch_size=batch_size,\n",
    "# #                                             num_workers=num_workers,\n",
    "#                                             collate_fn=collate_fn)\n",
    "    submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "\n",
    "    file_name_list = []\n",
    "    preds_array1 = np.empty((0, 12,512*512), dtype=np.long)\n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # (batch,channel,512,512)\n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            # # (batch,512,512)\n",
    "            # oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            # # resize (256 x 256)\n",
    "            # temp_mask = []\n",
    "            # for img, mask in zip(np.stack(imgs), oms):\n",
    "            #     transformed = transform(image=img, mask=mask)\n",
    "            #     # (256,256)\n",
    "            #     mask = transformed['mask']\n",
    "            #     temp_mask.append(mask)\n",
    "            #(batch,256,256)\n",
    "\n",
    "            # (batch,12,512,512)\n",
    "            oms=outs.detach().cpu().numpy()\n",
    "            #(batch,12,512*512)\n",
    "            oms = oms.reshape([oms.shape[0],oms.shape[1],512*512]).astype(int)\n",
    "            preds_array1 = np.vstack((preds_array1, oms))\n",
    "            # 배치가 쌓이다가 (배치+배치+배치+...,12,256*256) \n",
    "            # (전체데이터개수,12,256*256) 와 같이 된다.\n",
    "\n",
    "            # i는 batch 중에서 한개 그 한개의 파일이름을 배치만큼모아놓은리스트[f1,f2,f3,..fbatch]를 file_name_list에 넣는다.\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    \n",
    "    if i==0:\n",
    "        preds_array=preds_array1\n",
    "    else:\n",
    "        preds_array+=preds_array1\n",
    "    print(preds_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e479fb-7eae-4c0d-9232-9a6d217db6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 12, 262144)\n"
     ]
    }
   ],
   "source": [
    "print(preds_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22097c00-4147-42fd-8c9f-a08d569f2967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 262144)\n"
     ]
    }
   ],
   "source": [
    "preds_array1=preds_array/2 # 2로 나누는거는 2개 앙상블이므로므로\n",
    "preds_array1=np.argmax(preds_array1,1)\n",
    "print(preds_array1.shape)\n",
    "preds_array1=preds_array1.reshape([preds_array1.shape[0],512,512]).astype(int)\n",
    "transform = A.Compose([A.Resize(256, 256)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75dcff59-ea8d-4fb7-b99f-797cfd41c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 512, 512)\n",
      "(837, 3, 512, 512)\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 3 3 3]\n",
      "  [0 0 0 ... 3 3 3]\n",
      "  [0 0 0 ... 3 3 3]]]\n"
     ]
    }
   ],
   "source": [
    "preds_array1=preds_array1.reshape([837,512,512]).astype(int)\n",
    "print(preds_array1.shape)\n",
    "images=np.random.rand(837,3,512,512)\n",
    "print(images.shape)\n",
    "preds_array2=[]\n",
    "for images,masks in zip(images,preds_array1):\n",
    "    transformed = transform(image=images,mask=masks)\n",
    "    preds_array2.append(transformed['mask'])\n",
    "preds_array1=np.array(preds_array2)\n",
    "print(preds_array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9d826e-d779-4302-94f1-b17403e4f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 256, 256)\n",
      "(837, 65536)\n"
     ]
    }
   ],
   "source": [
    "print(preds_array1.shape)\n",
    "preds_array1=preds_array1.reshape([837,256*256]).astype(int)\n",
    "print(preds_array1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831d9182-cf25-4c3d-bb4d-fb77179ea4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae005743-08ef-471e-990e-07ad2fc55690",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0=pd.read_csv('/opt/ml/realcode/output/DeepLabV3Plus_ResNet152_Adamp_0.0001_marginx_plus_plus_notb_pseudo_cos_kfold0.csv')\n",
    "file_names=data0['image_id']\n",
    "for file_name, string in zip(file_names, preds_array1):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                ignore_index=True)\n",
    "submission.to_csv(os.path.join('./output','softensemble.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550978b-addf-4f11-a135-5a636ee8240e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
